{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Featurization\n",
    "Start with setup and imports (apologies for the vast quantity of imports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "#Build spark session, unable to get more than 8 cores for the second half of the semester\n",
    "#This configuration performs relatively well given that constraint. This application definitely wants more cores though. \n",
    "spark = SparkSession \\\n",
    "    .builder\\\n",
    "    .config(\"spark.executor.memory\", '14g') \\\n",
    "    .config(\"spark.executor.instances\", \"3\") \\\n",
    "    .config('spark.executor.cores', '2') \\\n",
    "    .config(\"spark.driver.memory\",'14g') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import io\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from pyspark.sql.functions import col, pandas_udf, PandasUDFType\n",
    "import os\n",
    "from spark_tensorflow_distributor import MirroredStrategyRunner\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, FloatType, StringType\n",
    "import re\n",
    "from pyspark.ml.feature import *\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from spark_tensorflow_distributor import MirroredStrategyRunner\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.classification import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in images to spark image Dataframe\n",
    "\n",
    "We also setup file locations. Everything should be accessable on the project folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data in project directory\n",
    "data_dir = \"/project/ds5559/BioNerds/ham\"\n",
    "os.chdir('/project/ds5559/BioNerds/ham')\n",
    "#Location of saved model from resnet notebook\n",
    "ResNet50_Folder = \"/project/ds5559/BioNerds/saved_data/ResNet50_testing/\"\n",
    "model_path = \"Final_model\"\n",
    "model_dir = os.path.join(ResNet50_Folder,model_path)\n",
    "\n",
    "#Location of saved mobilenet weights \n",
    "mobilenet_path = '/home/aaw3ff/HAM/Saved_Networks/mnet/mobilenet_tl-0001.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use recursive file lookup, files are in multiple directories\n",
    "images = spark.read.format(\"binaryFile\") \\\n",
    "  .option(\"pathGlobFilter\", \"*.jpg\") \\\n",
    "  .option(\"recursiveFileLookup\", \"true\") \\\n",
    "  .load(\"/project/ds5559/BioNerds/ham\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Featurize images\n",
    "\n",
    "Assign features to images using our resnet 50, xception, and MobileNet models. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Begin with defining UDFs to featurize our spark dataframe\n",
    "\n",
    "Each udf calls on a preprocess function to open the image, and a model function to return the model used to featurize the image. It returns an array of floats corresponding to the model weights the layer immediately preceding the top. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def model_fn_xcep():\n",
    "    import tensorflow as tf\n",
    "    \"\"\"\n",
    "    Return a topless Xception TL model with loaded weights.\n",
    "    \"\"\"\n",
    "    \n",
    "    #Define augmentation layer, no trainable weights but want identical structure\n",
    "    data_augmentation = tf.keras.Sequential([\n",
    "    tf.keras.layers.experimental.preprocessing.RandomFlip('horizontal_and_vertical'),\n",
    "    tf.keras.layers.experimental.preprocessing.RandomRotation(.99),\n",
    "    ])\n",
    "    \n",
    "    #Define global_average_layer for handling xception output\n",
    "    global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "    \n",
    "    #import Xception model with 10% dropout and preloaded imagenet weights \n",
    "    base_model = tf.keras.applications.Xception(\n",
    "    weights= 'imagenet',  # Load weights pre-trained on ImageNet.\n",
    "    input_shape=(150, 150, 3),\n",
    "    include_top=False,\n",
    "    classifier_activation = 'softmax'\n",
    "    )  \n",
    "    \n",
    "    #Define model \n",
    "    inputs = tf.keras.Input(shape=(150, 150, 3))\n",
    "    x = data_augmentation(inputs)\n",
    "    x = tf.keras.applications.xception.preprocess_input(x)\n",
    "    #Feed data to Xception\n",
    "    x = base_model(x)\n",
    "    x = global_average_layer(x)\n",
    "    #Add postprocessing layers, omit 7 class final layer\n",
    "    x = tf.keras.layers.Dense(1024,activation='relu')(x)\n",
    "    outputs = tf.keras.layers.Dense(256,activation='relu')(x)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    \n",
    "    #load weights from file, expect partial since we're missing the top layer\n",
    "    #Go ahead and mock me, I thought Xception and inceptionV3 were the same thing for a hot minute\n",
    "    #And now it won't load the weights if I change the filename :(\n",
    "    model.load_weights('/project/ds5559/BioNerds/Saved_Networks/xcep/inception_tl-0058.ckpt').expect_partial()\n",
    "\n",
    "    #Compile the model so that it can be used for prediction \n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(.001),\n",
    "                   loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                   metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def model_fn_mnet():\n",
    "    import tensorflow as tf\n",
    "    \"\"\"\n",
    "    Return a topless xception TL model with loaded weights.\n",
    "    \"\"\"\n",
    "    \n",
    "    #Define augmentation layer, no trainable weights but want identical structure\n",
    "    data_augmentation = tf.keras.Sequential([\n",
    "    tf.keras.layers.experimental.preprocessing.RandomFlip('horizontal_and_vertical'),\n",
    "    tf.keras.layers.experimental.preprocessing.RandomRotation(.99),\n",
    "    ])\n",
    "    \n",
    "    #import mobilenet model with 10% dropout and preloaded imagenet weights \n",
    "    mnet2 = tf.keras.applications.MobileNet(\n",
    "        input_shape=None,\n",
    "        alpha=1.0,\n",
    "        depth_multiplier=1,\n",
    "        dropout=0.1,\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        input_tensor=None,\n",
    "        pooling='max',\n",
    "        classifier_activation=\"softmax\"\n",
    "    )\n",
    "    \n",
    "    #Define model \n",
    "    inputs = tf.keras.Input(shape=(224, 224, 3))\n",
    "    #Augmentation layer used in training\n",
    "    x = data_augmentation(inputs)\n",
    "    #Preprocess and feed to mobilenet\n",
    "    x = tf.keras.applications.mobilenet.preprocess_input(x)\n",
    "    x = mnet2(x)\n",
    "    #Postprocessing layers, omit final layer, see mobilenet notebook for details\n",
    "    x = tf.keras.layers.Dense(units = 1024, activation = 'relu')(x)\n",
    "    outputs = tf.keras.layers.Dense(units = 256, activation = 'relu')(x)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    \n",
    "    #load weights from file, expect partial since we're missing the top layer\n",
    "    model.load_weights('/project/ds5559/BioNerds/Saved_Networks/mnet/mobilenet_tl-0001.ckpt').expect_partial()\n",
    "\n",
    "    #Compile the model so that it can be used\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(.001),\n",
    "                   loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                   metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(content):\n",
    "    \"\"\"\n",
    "    Preprocesses raw image bytes for prediction, resize to 224 for mobilenet/resnet.\n",
    "    \"\"\"\n",
    "    img = Image.open(io.BytesIO(content)).resize([224, 224])\n",
    "    arr = img_to_array(img)\n",
    "    return preprocess_input(arr)\n",
    "\n",
    "def preprocess_xcep(content):\n",
    "    \"\"\"\n",
    "    Preprocesses raw image bytes for prediction, resize to 150 for xception.\n",
    "    \"\"\"\n",
    "    img = Image.open(io.BytesIO(content)).resize([150, 150])\n",
    "    arr = img_to_array(img)\n",
    "    return preprocess_input(arr)\n",
    "\n",
    "def featurize_series(model, content_series):\n",
    "    \"\"\"\n",
    "    Take the model and output the layer below the top as features. \n",
    "    \"\"\"\n",
    "    input = np.stack(content_series.map(preprocess))\n",
    "    preds = model.predict(input)\n",
    "    #Flatten multidimensional model outputs\n",
    "    output = [p.flatten() for p in preds]\n",
    "    return pd.Series(output)\n",
    "\n",
    "def featurize_series_xcep(model, content_series):\n",
    "    \"\"\"\n",
    "    Take the model and output the layer below the top as features. \n",
    "    \"\"\"\n",
    "    input = np.stack(content_series.map(preprocess_xcep))\n",
    "    preds = model.predict(input)\n",
    "    #Flatten multidimensional model outputs\n",
    "    output = [p.flatten() for p in preds]\n",
    "    return pd.Series(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn_r50():\n",
    "    \"\"\"\n",
    "    Returns a New ResNet50 transfer learning model with top layer removed.\n",
    "    See resnet 50 notebook for the creation of said model\n",
    "    \"\"\"\n",
    "    model = tf.keras.models.load_model(model_dir)\n",
    "    new_model = tf.keras.models.Sequential(model.layers[:-1])\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/spark/python/pyspark/sql/pandas/functions.py:386: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  \"in the future releases. See SPARK-28264 for more details.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "@pandas_udf('array<float>', PandasUDFType.SCALAR_ITER)\n",
    "#Your own code recipe for this specifies type, stop yelling at me DataBricks\n",
    "def featurize_mnet_udf(content_series_iter):\n",
    "    '''\n",
    "    This method is a Scalar Iterator pandas UDF wrapping our featurization function.\n",
    "    The decorator specifies that this returns a Spark DataFrame column of type ArrayType(FloatType).\n",
    "  \n",
    "    :param content_series_iter: This argument is an iterator over batches of data, where each batch\n",
    "                              is a pandas Series of image data.\n",
    "    '''\n",
    "  # With Scalar Iterator pandas UDFs, we can load the model once and then re-use it\n",
    "  # for multiple data batches.  This amortizes the overhead of loading big models.\n",
    "    model = model_fn_mnet()\n",
    "    for content_series in content_series_iter:\n",
    "        yield featurize_series(model, content_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pandas_udf('array<float>', PandasUDFType.SCALAR_ITER)\n",
    "#Your own code recipe for this specifies type, stop yelling at me DataBricks\n",
    "def featurize_r50_udf(content_series_iter):\n",
    "    '''\n",
    "    This method is a Scalar Iterator pandas UDF wrapping our featurization function.\n",
    "    The decorator specifies that this returns a Spark DataFrame column of type ArrayType(FloatType).\n",
    "  \n",
    "    :param content_series_iter: This argument is an iterator over batches of data, where each batch\n",
    "                              is a pandas Series of image data.\n",
    "    '''\n",
    "  # With Scalar Iterator pandas UDFs, we can load the model once and then re-use it\n",
    "  # for multiple data batches.  This amortizes the overhead of loading big models.\n",
    "    model = model_fn_r50()\n",
    "    for content_series in content_series_iter:\n",
    "        yield featurize_series(model, content_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pandas_udf('array<float>', PandasUDFType.SCALAR_ITER)\n",
    "#Your own code recipe for this specifies type, stop yelling at me DataBricks\n",
    "def featurize_xcep_udf(content_series_iter):\n",
    "    '''\n",
    "    This method is a Scalar Iterator pandas UDF wrapping our featurization function.\n",
    "    The decorator specifies that this returns a Spark DataFrame column of type ArrayType(FloatType).\n",
    "  \n",
    "    :param content_series_iter: This argument is an iterator over batches of data, where each batch\n",
    "                              is a pandas Series of image data.\n",
    "    '''\n",
    "  # With Scalar Iterator pandas UDFs, we can load the model once and then re-use it\n",
    "  # for multiple data batches.  This amortizes the overhead of loading big models.\n",
    "    model = model_fn_xcep()\n",
    "    for content_series in content_series_iter:\n",
    "        yield featurize_series_xcep(model, content_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call said udfs\n",
    "\n",
    "Here, we call our UDFS to featurize the images in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a udf that takes a path and returns the image ID for joining with other dataframes\n",
    "\n",
    "pathToID = udf(lambda z: toClass(z), StringType())\n",
    "spark.udf.register(\"pathToID\", pathToID)\n",
    "def toClass(s):\n",
    "    #just use regex to grab class from filename\n",
    "    p = re.compile(\"ISIC_[0-9]*\")\n",
    "    result = p.search(s)\n",
    "    return(result.group(0))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Featurize them images, theres probably a more efficient/elegant way to do this\n",
    "mnet_df = images.repartition(12).select(col(\"path\"), featurize_mnet_udf(\"content\").alias(\"mnet_features\"))\n",
    "r50_df = images.repartition(12).select(col(\"path\"), featurize_r50_udf(\"content\").alias(\"r50_features\"))\n",
    "xcep_df = images.repartition(12).select(col(\"path\"), featurize_xcep_udf(\"content\").alias(\"xcep_features\"))\n",
    "\n",
    "#Add ID column for dataframe joins\n",
    "mnet_df = mnet_df.withColumn('ID', pathToID('path'))\n",
    "r50_df = r50_df.withColumn('ID', pathToID('path'))\n",
    "xcep_df = xcep_df.withColumn('ID', pathToID('path'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+--------------------+--------------------+\n",
      "|          ID|       mnet_features|        r50_features|       xcep_features|\n",
      "+------------+--------------------+--------------------+--------------------+\n",
      "|ISIC_0024536|[0.0, 0.0, 0.0, 0...|[0.03389938, 0.0,...|[0.0, 0.0, 0.0, 0...|\n",
      "|ISIC_0024549|[0.0, 0.19771415,...|[0.0, 0.0, 0.0, 0...|[0.0, 0.0, 0.0, 0...|\n",
      "|ISIC_0024738|[0.0, 0.0, 0.0, 0...|[0.31574133, 0.0,...|[0.0, 0.0, 0.0, 0...|\n",
      "|ISIC_0024766|[0.0, 0.0, 0.0, 0...|[1.1685055, 0.0, ...|[0.0, 0.0, 0.0, 0...|\n",
      "|ISIC_0025187|[0.0, 0.0, 0.0, 0...|[0.1397682, 0.0, ...|[0.0, 0.0, 0.0, 0...|\n",
      "+------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Join the features dataframes\n",
    "#Drop path as it is not needed\n",
    "all_feats = mnet_df.join(r50_df, on = 'ID', how = 'inner')\n",
    "all_feats = all_feats.join(xcep_df, on = 'ID', how = 'inner').drop('path')\n",
    "\n",
    "all_feats.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integration with metadata\n",
    "\n",
    "There is also a file with patient metadata for the patients. We will integrate the relevant portions with our features. The metadata contains the diagnosis (our variable of interest) as well as ptient age, patient sex, and location of lesion. Dx-type is included, but is excluded as a predictive variable since it is not a predictor available at time of diagnosis. Ideally, we want to have some idea what these lesions are before we cut them off entirely. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV file\n",
    "meta_df = spark.read.csv('/project/ds5559/BioNerds/ham/HAM10000_metadata.csv', header = True, inferSchema = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+---+-------+----+----+------------+\n",
      "|  lesion_id|    image_id| dx|dx_type| age| sex|localization|\n",
      "+-----------+------------+---+-------+----+----+------------+\n",
      "|HAM_0000118|ISIC_0027419|bkl|  histo|80.0|male|       scalp|\n",
      "|HAM_0000118|ISIC_0025030|bkl|  histo|80.0|male|       scalp|\n",
      "|HAM_0002730|ISIC_0026769|bkl|  histo|80.0|male|       scalp|\n",
      "|HAM_0002730|ISIC_0025661|bkl|  histo|80.0|male|       scalp|\n",
      "|HAM_0001466|ISIC_0031633|bkl|  histo|75.0|male|         ear|\n",
      "+-----------+------------+---+-------+----+----+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "meta_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join Metadata and Features\n",
    "\n",
    "We've already convieniently converted out paths to image ids, so here we'll just join "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = all_feats.join(meta_df, all_feats.ID == meta_df.image_id, 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: string (nullable = true)\n",
      " |-- mnet_features: array (nullable = true)\n",
      " |    |-- element: float (containsNull = true)\n",
      " |-- r50_features: array (nullable = true)\n",
      " |    |-- element: float (containsNull = true)\n",
      " |-- xcep_features: array (nullable = true)\n",
      " |    |-- element: float (containsNull = true)\n",
      " |-- lesion_id: string (nullable = true)\n",
      " |-- image_id: string (nullable = true)\n",
      " |-- dx: string (nullable = true)\n",
      " |-- dx_type: string (nullable = true)\n",
      " |-- age: double (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- localization: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize the output of our UDFs \n",
    "\n",
    "Explicit vector UDT types are required for vector assembler and downstream work. We then drop the non-useful portions of the dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_vector = udf(lambda a: Vectors.dense(a), VectorUDT())\n",
    "\n",
    "\n",
    "all_df = all_df.select(\"*\", to_vector(\"mnet_features\").alias(\"mnet_vec\"))\n",
    "all_df = all_df.select(\"*\", to_vector(\"r50_features\").alias(\"r50_vec\"))\n",
    "all_df = all_df.select(\"*\", to_vector(\"xcep_features\").alias(\"xcep_vec\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = all_df.drop('path', 'r50_features', 'xcep_features', 'mnet_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: string (nullable = true)\n",
      " |-- lesion_id: string (nullable = true)\n",
      " |-- image_id: string (nullable = true)\n",
      " |-- dx: string (nullable = true)\n",
      " |-- dx_type: string (nullable = true)\n",
      " |-- age: double (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- localization: string (nullable = true)\n",
      " |-- mnet_vec: vector (nullable = true)\n",
      " |-- r50_vec: vector (nullable = true)\n",
      " |-- xcep_vec: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ID: string, lesion_id: string, image_id: string, dx: string, dx_type: string, age: double, sex: string, localization: string, mnet_vec: vector, r50_vec: vector, xcep_vec: vector]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_df.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save featurized images for future use\n",
    "\n",
    "Write a parquet with the featurized images and associated metadata. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First rule of grading this project, do not ask about clean_features.parquet1\n",
    "all_df.write.parquet(\"clean_features.parquet2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Did is work?\n",
    "new = spark.read.parquet(\"clean_features.parquet2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+---+---------+----+------+---------------+--------------------+--------------------+--------------------+\n",
      "|          ID|  lesion_id|    image_id| dx|  dx_type| age|   sex|   localization|            mnet_vec|             r50_vec|            xcep_vec|\n",
      "+------------+-----------+------------+---+---------+----+------+---------------+--------------------+--------------------+--------------------+\n",
      "|ISIC_0024306|HAM_0000550|ISIC_0024306| nv|follow_up|45.0|  male|          trunk|[0.0,0.0,0.0,0.0,...|[0.24354705214500...|[0.0,0.0,0.0,0.0,...|\n",
      "|ISIC_0024387|HAM_0004156|ISIC_0024387| nv|    histo|65.0|female|lower extremity|[0.0,1.3627424240...|[0.0,0.0,0.0,0.0,...|[0.0,0.0,0.0,0.0,...|\n",
      "|ISIC_0024397|HAM_0001501|ISIC_0024397| nv|follow_up|65.0|  male|          trunk|[0.0,0.0,0.0,0.0,...|[1.18369388580322...|[0.0,0.0,0.0,0.0,...|\n",
      "|ISIC_0024551|HAM_0002629|ISIC_0024551| nv|follow_up|50.0|  male|        abdomen|[0.0,0.0,0.0,0.0,...|[0.01115676853805...|[0.0,0.0,0.0,0.0,...|\n",
      "|ISIC_0024700|HAM_0001726|ISIC_0024700|mel|    histo|35.0|female|          trunk|[0.0,2.0978577136...|[0.0,0.0,0.0,0.0,...|[0.0,0.0,0.0,0.0,...|\n",
      "+------------+-----------+------------+---+---------+----+------+---------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#yep\n",
    "new.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS 5110",
   "language": "python",
   "name": "ds5110"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
